# 2.3 얼굴 표정과 감성 인식

## FACS (Facial Action Coding System)

**FACS**는 심리학자 Paul Ekman과 Wallace V. Friesen이 개발한 얼굴 표정을 분석하는 시스템으로, 얼굴 표정을 Action Unit (AU)이라고 불리는 일련의 근육 움직임으로 분류하여 분석합니다.

### Action Unit (AU)
- **정의**: 얼굴 근육 움직임의 기본 단위
- **특징**: 
  - 각 AU는 특정 근육 또는 근육 그룹의 움직임에 해당
  - AU 번호와 함께 강도(A-E: 미세-최고)를 알파벳 등급으로 표시

### 주요 AU 목록
- **눈썹 움직임**: AU 1-4
- **눈 움직임**: AU 5-7, 25-30
- **코, 볼 움직임**: AU 8-10
- **입술 움직임**: AU 11-24

### 기본 감성과 AU
Ekman의 6가지 기본 감성은 특정 AU 조합으로 표현됩니다:
- **행복**: AU 6 (뺨 올리기) + AU 12 (입꼬리 올리기)
- **슬픔**: AU 1 (안쪽 눈썹 올리기) + AU 4 (눈썹 내리기) + AU 15 (입꼬리 내리기)
- **놀람**: AU 1 + AU 2 (바깥쪽 눈썹 올리기) + AU 5 (윗눈꺼풀 올리기) + AU 26 (턱 내리기)
- **두려움**: AU 1 + AU 2 + AU 4 + AU 5 + AU 7 (눈꺼풀 조이기) + AU 20 (입 옆으로 늘리기)
- **분노**: AU 4 + AU 5 + AU 7 + AU 23 (입술 조이기)
- **혐오**: AU 9 (코 주름잡기) + AU 15 + AU 16 (아랫입술 내리기)

## 기본 감성의 역사적 연구

- **Duchenne (1862)**: 
  - 근육 전기 자극을 통해 얼굴 근육의 해부학적 움직임 파악
  - 13개 감성으로 분류

- **Darwin (1872)**: 
  - 진화론에 근거하여 표정을 분류
  - 감성과 표정이 의사소통 수단으로 사용된다고 주장
  - 8개 감성 분류

- **Tomkins (1962)**: 
  - 표정에 내적 정보가 있다고 주장
  - 감성은 학습 없이 유전적으로 동일한 표정을 가진다고 주장
  - 9개 감성 분류

- **Ekman (1969)**: 
  - 범문화적 기본 감성 정의
  - 다양한 문화권에서 6가지 기본 감성(행복, 슬픔, 놀람, 두려움, 분노, 혐오)을 동일하게 인식함을 확인

## 얼굴 표정과 감성의 관계

- **감성 표현**: 얼굴 표정은 비언어적 의사소통의 중요한 수단

- **보편성**: 
  - 서로 다른 문화권에서도 기본 감성을 묘사하는 얼굴 사진을 보고 동일하게 감성을 인지
  - 유인원과 인간의 표정 유사성은 진화적 연속성을 시사

- **뇌의 반응**: 
  - 표정은 감성의 생체 유기적 반응 중 하나
  - 의식적/무의식적 뇌 반응과 관련됨

## 얼굴 표정의 해부학적 특징

- **표정근**: 
  - 얼굴 표정을 만드는 근육
  - 13개 근육이 대표적으로 사용됨

- **관련 신경**:
  - **삼차 신경**: 얼굴 감각 정보 담당
  - **안면 신경**: 얼굴 근육 운동 정보 담당
  - **동안 신경**: 눈 움직임, 홍채 조절 기능 담당

- **수의적/불수의적 조절**: 
  - 얼굴 표정 근육은 수의적(의식적) 조절과 불수의적(무의식적) 조절이 모두 가능

## 얼굴의 무의식적 근육과 미세 표정

- **불수의근 특징**: 
  - 의지와 상관없이 움직이는 근육
  - 가짜 감성 표현 불가능

- **미세 표정 (Micro-expression)**: 
  - 0.01~0.5초 사이에 무의식적으로 나타나는 미세한 표정
  - 감정 인식 정확도 향상에 도움
  - 진실/거짓 표정 구분에 활용

- **뒤센 미소 (Duchenne Smile)**: 
  - 진정한 미소를 나타냄
  - 대협골근(Zygomatic major muscle)과 안와근(Orbicularis oculi muscle)이 함께 수축해야 함
  - 진짜 행복을 나타내는 유일한 표정으로 간주됨

## 표정과 제스처 간 일치성

- **Congruency(일치성)**: 
  - 감정 표현은 표정과 제스처가 일치해야 함
  - 같은 표정이라도 제스처가 다르면 다른 감성으로 느껴질 수 있음
  - 불일치할 경우 관찰자에게 불편함이나 불신을 유발할 수 있음

## 얼굴 근육 움직임 측정 방법

- **Action Unit (AU)**: 
  - 표정을 정량화하기 위한 근육 움직임 측정 단위
  - 23개 핵심 조합 움직임 정의

- **측정 방법**:
  - **FAST (Facial Affect Scoring Technique)**: 
    - 얼굴 요소를 서술하여 감성 구별
    - 특정 얼굴 영역의 변화에 초점을 맞춤
  
  - **FACS (Facial Action Coding System)**: 
    - 얼굴 움직임을 체계화하여 분석
    - 해부학 기반 AU 분석
    - 객관적이고 정밀한 코딩 시스템

- **한계점**: 
  - 미세한 변화 측정 어려움
  - 근육 긴장도/피부색 등 고려하지 못함
  - 개인차와 문화적 맥락 반영의 어려움

## AU 강도와 표정 변화

- **강도 변화의 영향**:
  - AU 강도가 증가하면 감성 강도도 함께 증가
  - 주변 AU가 합세하여 표정 강도 증가
  - 미미한 AU 변화로도 감정 인식에 영향을 줄 수 있음

- **강도 평가 척도**:
  - A: 미세(흔적) 수준
  - B: 약함
  - C: 뚜렷함
  - D: 강함
  - E: 최고 수준

## 2차원 감성 매핑

- **감성의 2차원 모델**:
  - **Arousal(각성)**: 감정의 활성화 정도(낮음-높음)
  - **Pleasure/Valence(쾌-불쾌)**: 감정의 긍정-부정 정도

- **매핑 방법**:
  - AU 활성화 패턴이 2차원 공간에서 특정 영역에 위치
  - 행복: 높은 쾌, 중간 각성
  - 분노: 높은 불쾌, 높은 각성
  - 슬픔: 높은 불쾌, 낮은 각성
  - 평온: 중간 쾌, 낮은 각성

- **응용**:
  - 감정 인식 시스템의 분류 기준으로 활용
  - 연속적인 감정 상태 표현 가능
  - 복합 감정의 표현과 이해에 유용

## MPEG-4 Point와 얼굴 랜드마크

- **MPEG-4 Point**:
  - 국제 표준화된 얼굴 랜드마크 시스템
  - 오디오, 비디오, 3D 그래픽 등 멀티미디어 통신에 사용
  - ISO/IEC 국제 표준으로 제정

- **FAP (Face Animation Parameter)**:
  - MPEG-4에서 얼굴 애니메이션을 위해 지정한 랜드마크
  - 얼굴 영역에 따라 68개 지점을 10개 그룹으로 재정렬
  - 얼굴 애니메이션의 국제 표준으로 사용

- **FDP (Face Definition Parameter)**:
  - 얼굴의 특징점과 윤곽을 정의
  - 3D 모델링과 애니메이션에 활용

## AU와 MPEG-4 매핑

- **목적**: 
  - 표정 인식(AU)과 표현(MPEG-4)을 일원화
  - 감성 인식과 가상 캐릭터 표현 기술의 통합

- **방법**: 
  - AU와 MPEG-4 랜드마크 간의 차이를 매핑하여 호환성 확보
  - AU 조합을 FAP 값으로 변환하는 파라미터 정의
  - 역변환을 통해 FAP 데이터에서 AU 추출 가능

- **활용**:
  - 실시간 표정 인식 결과를 가상 아바타에 즉시 적용
  - 애니메이션과 실제 표정 간의 자연스러운 연결

## 표정 인식 프로그램 과정

1. **비디오/웹캠 데이터 입력**: 실시간 또는 녹화된 영상 데이터 획득
2. **프레임 추출**: 영상에서 개별 프레임 추출
3. **얼굴 랜드마크 추출**: 얼굴 영역 감지 및 주요 특징점 추출
4. **상대 좌표 변환**: 크기와 회전에 불변하는 좌표계로 변환
5. **AU 구성**: 랜드마크의 상대적 위치 변화를 AU로 매핑
6. **AU Centroid(중심점) 추출**: 각 AU의 대표 값 계산
7. **Centroid 변화율 계산**: 기준점 대비 변화량 측정
8. **감성 판단 (Rule base)**: 사전 정의된 규칙에 따라 감성 분류

- **Rule base**: 
  - Ekman의 기본 감성 규칙을 기반으로 함
  - 판단 기준치는 실험을 통해 설정 필요
  - 머신러닝 기법으로 정확도 향상 가능

## 표정의 개인차

- **원인**: 
  - 개인의 표현 성향(표현형/내향형)
  - 표정 강도의 기준 차이
  - 문화적 배경에 따른 표정 표현 규범
  - 개인적 표정 습관과 특이성
  - 얼굴 구조의 차이

- **극복 방안**: 
  - 개인별 베이스라인 측정 후 상대적 변화 분석
  - 적응형 알고리즘으로 개인 특성 학습
  - 문화적 맥락을 고려한 해석 모델
  - 다양한 얼굴 형태에 대한 트레이닝 데이터 확보
  - 시간 경과에 따른 표정 패턴 분석

## 휴먼 아바타와 감성 표현

- **기술적 응용**:
  - 얼굴 근육 움직임 수치를 아바타 모델링에 직접 적용
  - 실시간 감정 표현이 가능한 버추얼 휴먼 개발
  - 표정과 감성의 세밀한 연결을 통한 몰입감 증대

- **구현 방법**:
  - FACS 기반 리깅(Rigging) 시스템 구축
  - 근육 시뮬레이션을 통한 사실적 표정 재현
  - 실시간 감성 데이터와 아바타 표정의 동기화

- **활용 분야**:
  - 영화와 게임의 디지털 캐릭터
  - 가상 비서 및 AI 인터페이스
  - 원격 회의 및 소통 도구
  - 가상현실(VR) 소셜 플랫폼

## 표정 인식 산업과 응용

- **3대 감성 컴퓨팅 유망 기술**: 
  - 감성 스피치: 음성을 통한 감정 인식
  - 감성 제스처: 신체 움직임을 통한 감정 인식
  - 감성 표정 인식: 얼굴 표정을 통한 감정 인식

- **주요 기업과 기술**:
  - **인텔 RealSense**: 
    - 3D 깊이 센서 기반 미세 움직임 측정
    - 실시간 감성 인식 API 제공
  
  - **페이스북 DeepFace**: 
    - 딥러닝 기반 얼굴 근육 움직임 분석
    - 97.35% 인식 정확도(인간 수준 97.53%)
  
  - **애플 3D ANIMOJI**: 
    - 50개 이상 얼굴 근육 매핑
    - AR 기술과 결합한 감정 표현 아바타
  
  - **구글 Google Inception**: 
    - 표정 인식을 통한 환자 질병 진단 지원
    - 의료 영상과 표정 데이터 통합 분석

- **감성 인식 오픈소스 프로젝트 기업**: 
  - Emotient: 소비자 감정 반응 분석
  - IBM: Watson Visual Recognition API
  - Microsoft: Face API와 Emotion API
  - Affectiva: 자동차 운전자 감정 모니터링
  - Nviso: 금융 서비스 사용자 경험 분석
  - Kairos: 마케팅 및 소매업 고객 분석

- **미래 발전 방향**:
  - 다중 모달리티(표정, 음성, 생체신호) 통합 분석
  - 문화적 맥락을 고려한 인식 시스템
  - 감성 기반 인간-컴퓨터 상호작용 발전
  - 정신 건강 및 의료 진단 분야 응용 확대
